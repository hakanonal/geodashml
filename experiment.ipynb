{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation\n",
    "\n",
    "This notebook's purpose is to easily experiment some chunks of code. There will be nothing here that actually works. The main purpose is to show my idea walkthourgs. When time passes I am expecting to copy-paste the working code from here to original .py script files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Read screen from a web page experiments](https://github.com/hakanonal/geodashml/projects/1#card-37027228)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26.04.2020 - I have initally googled and bumped into [this](https://stackoverflow.com/questions/38568804/python-open-html-file-take-screenshot-crop-and-save-as-image) and [this](https://stackoverflow.com/questions/38568804/python-open-html-file-take-screenshot-crop-and-save-as-image). \n",
    "\n",
    "- Some fooling around with these ideas. \n",
    "- it appears that selenium is the right tool to get the screen shot. I still not sure if the scracth version of the game is going to provide the continoues images during playing the game. After all I do not have any idea what the game's runtime. we need to test more if we can get screenshots in a loop so that we can get series of screenshot images to feed into the network.\n",
    "- I have bumped into selenium installation problem, currentlly trying to resolve, \"Message: 'Google\\ Chrome.app' executable needs to be in PATH. Please see https://sites.google.com/a/chromium.org/chromedriver/home\" error message \n",
    "- This [page](https://www.selenium.dev/documentation/en/webdriver/driver_requirements/) seems to be helped a lot. It is the official selenium documentaion page that explains working principle and driver installation. It also re-directs to google's own [page](https://sites.google.com/a/chromium.org/chromedriver/downloads) to download the driver. I have matched my installed chrome application version with the driver version. I have also needed my mac security setting to allow un-trusted code to execute. I have moved it to /usr/local/bin folder. I choose to trust [this](https://sites.google.com/a/chromium.org/chromedriver/home) guys. Consider [security](https://sites.google.com/a/chromium.org/chromedriver/security-considerations) also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome(executable_path=\"/usr/local/bin/chromedriver\", chrome_options=options)\n",
    "\n",
    "driver.get('https://scratch.mit.edu/projects/105500895/fullscreen/')\n",
    "image = driver.find_element_by_id('view').screenshot_as_png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- selenium seems to be working. Now we need if we can get the image that we want. It seems not. It is not rendered properlly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27.04.2020 - Continue to explore selenium api\n",
    "- after a blank page I will try different elements to visualize, there may be problem with the flash or html5 or javascript (non-static) elements.\n",
    "- started to read selenium [webdriver documantation](https://www.selenium.dev/documentation/en/webdriver/waits/) in detail. \"Waits\" section seems to be promissing\n",
    "- using the WebDriverWait api has solved the problem.\n",
    "- waiting is solved the problem but it works inconsistantly. after wating I may need to call find_element function second time. I am planing to fool arround with this code more.\n",
    "- driver.quit() call seems to be important, otherwise the application stays open. so I have put it into try: finally block. to execute every time.\n",
    "- trying to understand which element to wait for specificlly this project\n",
    "- to watch the progress we can get rid of \"options.add_argument('headless')\" statement so a new window will open\n",
    "- so after fittling around a little bit more, I have finnally clicked the green flag element an get to the main menu, now I plan to start game and get the updated series images in a loop.\n",
    "- I am trying to find the right element to start the game by clicking. can not find where to click\n",
    "- I have founded the element that I have trying to find using find_element**s**_by_css_selector which returns a list. however find_element_by_css_selector this version brings only one element.\n",
    "- The selenium api is working perfectlly fine. My problem is in scratch. Normally user clicks on the head of the level name or even pressing space is enough, however I can not which element is actually catching that key stroke. I need to find a way to debug the scratch anc find the element that catches the key presses.\n",
    "- In scratch all visible game elements are in a single element tag named as canvas. I have tried to press space all the parent elements of the canvas however I always get the following error message \"Message: element not interactable\"\n",
    "- IS it possible to overcome this problem by getting the root docuement try to send the key command? Checking more on selenium documents. Yes it did work. find element by tag and the tag name is \"html\" Yeah!\n",
    "- Now I can consantrate on getting series of images and series of sending keys...\n",
    "- Just to see if it is working I have waited 1 second and take 3 different snapshots. You can see that our fellow moved forward on the last image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from IPython.display import Image\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome(executable_path=\"/usr/local/bin/chromedriver\", chrome_options=options)\n",
    "\n",
    "try:\n",
    "    driver.get('https://scratch.mit.edu/projects/105500895/fullscreen/')\n",
    "    wait = WebDriverWait(driver,timeout=10)\n",
    "    wait.until(lambda d: d.find_element_by_css_selector(\".stage_green-flag-overlay_gNXnv img\")) #wait until green flag appears\n",
    "\n",
    "    flag_element = driver.find_element_by_css_selector(\".stage_green-flag-overlay_gNXnv img\")\n",
    "    flag_element.click()\n",
    "\n",
    "    wait.until(lambda d: d.find_element_by_css_selector(\".monitor_monitor-container_2J9gl\")) #wait until score board appears\n",
    "\n",
    "    driver.implicitly_wait(10)\n",
    "    html = driver.find_element_by_tag_name(\"html\")\n",
    "    html.send_keys(Keys.SPACE)\n",
    "    wait.until(lambda d: d.find_element_by_css_selector(\".monitor_value_3Yexa\")) #wait until score monitor element appears. \n",
    "\n",
    "    image0 = driver.find_element_by_css_selector(\"#view\").screenshot_as_png\n",
    "    driver.implicitly_wait(1)\n",
    "    image1 = driver.find_element_by_css_selector(\"#view\").screenshot_as_png\n",
    "    driver.implicitly_wait(1)\n",
    "    image2 = driver.find_element_by_css_selector(\"#view\").screenshot_as_png\n",
    "    driver.implicitly_wait(1)\n",
    "\n",
    "\n",
    "finally:\n",
    "    #print(\"ok\")\n",
    "    driver.quit()\n",
    "\n",
    "Image(image0)\n",
    "Image(image1)\n",
    "Image(image2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28.04.2020 - Let's put this into loop so that we can get series of images.\n",
    "\n",
    "- that was easy. using [this](https://stackoverflow.com/questions/11854847/how-can-i-display-an-image-from-a-file-in-jupyter-notebook)\n",
    "- ok then we need this image to be numpy array. go...\n",
    "- I have checked [this](https://stackoverflow.com/questions/57318892/convert-base64-encoded-image-to-a-numpy-array) site. I am getting base 64 screenshot and feed it in to io.BytsIO, however I get error message \"a bytes-like object is required, not 'str'\"\n",
    "- When I encode it I get the following message. \"cannot identify image file <_io.BytesIO object at 0x111db39e8>\" [ref](https://stackoverflow.com/questions/42612002/python-sockets-error-typeerror-a-bytes-like-object-is-required-not-str-with/42612820)\n",
    "- It seems that I need to understand better what is base64 what is string what does it represent. Will do some research\n",
    "- Well I did not understand but instead of getting the screen shot as base 64 I have get the screenshot as png and used same code mentioned [here](https://stackoverflow.com/questions/57318892/convert-base64-encoded-image-to-a-numpy-array). And it worked!\n",
    "- I have showed the series of images in a plot to see they are working fine. Yeah!!! That was quiker then expected...\n",
    "- as long as I see This means I have completed the first [task](https://github.com/hakanonal/geodashml/projects/1#card-37027228) of my [project](https://github.com/hakanonal/geodashml/projects/1).\n",
    "- the last image has FPS metric. I am not sure why it appeared.\n",
    "- I am worried about frame rate of this loop. I mean every loop getting screenshot and converting to numpy array takes time. Moreover when we move this code to the real script there will be additional score check and traning process will involve. So every screenshot action will be longer. I  will call this frame rate: the time that passes between takining the screen shot and getting the result from the network. To get a littile bit sense of it I have increased the samlpe loop size. It does not seem promissing. If in the future game performance is not going to be increase I will consider if one of the reason may be the low frame rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from PIL import Image\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "driver = webdriver.Chrome(executable_path=\"/usr/local/bin/chromedriver\", chrome_options=options)\n",
    "\n",
    "try:\n",
    "    driver.get('https://scratch.mit.edu/projects/105500895/fullscreen/')\n",
    "    wait = WebDriverWait(driver,timeout=10)\n",
    "    wait.until(lambda d: d.find_element_by_css_selector(\".stage_green-flag-overlay_gNXnv img\")) #wait until green flag appears\n",
    "\n",
    "    flag_element = driver.find_element_by_css_selector(\".stage_green-flag-overlay_gNXnv img\")\n",
    "    flag_element.click()\n",
    "\n",
    "    wait.until(lambda d: d.find_element_by_css_selector(\".monitor_monitor-container_2J9gl\")) #wait until score board appears\n",
    "\n",
    "    driver.implicitly_wait(20)\n",
    "    html = driver.find_element_by_tag_name(\"html\")\n",
    "    html.send_keys(Keys.SPACE)\n",
    "    wait.until(lambda d: d.find_element_by_css_selector(\".monitor_value_3Yexa\")) #wait until score monitor element appears. \n",
    "\n",
    "    fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(20, 10))\n",
    "    numpt_test_images = []\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            image_png = driver.find_element_by_css_selector(\"#view\").screenshot_as_png\n",
    "            img = Image.open(io.BytesIO(image_png))\n",
    "            numpt_test_images.append(np.asarray(img))\n",
    "            axes[j][i].imshow(numpy_array)\n",
    "            print(numpy_array.shape)\n",
    "    plt.show()\n",
    "\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Build Agent Class](https://github.com/hakanonal/geodashml/projects/1#card-37027254) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "29.04.2020 - Starting the agent class godspeed!\n",
    "\n",
    "- My previous project (which is curentlly not on github) get me a lot of insight about reinfocement learning. So I will be copying a lot of code from there. Th emain structure was the play function to advance the step, choosing the strategy (exploit or explore) and get the next action and update the q-value accoring to new step. Also agent class should initilize the network.\n",
    "- To test the agent class I am guessing that I am also going to look into the [Game Environment class task](https://github.com/hakanonal/geodashml/projects/1#card-37027280)\n",
    "- So the network structure is very important. I can not remember right now but onve I have read an article that someone has made a super mario agent with convolutional network ending up with multiple dense layer and giving the prediction of action to give. In our example the action set very very easy press space or not. dude this is so simple. So our state is going to be the numpy array from the screen shot we get earlier and the action is going to be single nouron. To construct the model I need to figure out the shapes of the input. So I will do that in aboove section. Well the shape is (600,785,4) it is 4 channel picture. Very high quality I do not think that we need that much hish quality picture but we will see. It is going to be wild advanture.\n",
    "- I am going to copy paste some code from my earlier learning sessions. My initial network will proablly overkill, but the idea is starting with convolutional layers following with pooling in the end flatten it with 2 dense layer and ending with single nuoron sigmoid layer. All middle dense layers are relu. This was genereally best pratice among the reading I have made so far. I may tweak the layer counts of conv and desse layers, nouron/filter sizes and shapes later. I think the padding should not be same because the input shape is already huge. It feels like it should be narrowed down quicklly.\n",
    "- I little bit testing is going on if agent class is ok, how model summary looks like. Make some predictions with checking if class helper methods are working fine...\n",
    "- I have copy paste the train and update methods from my previous project did some minor changes to fit it into this project accoring to the input and put shapes.\n",
    "- Ok during re forming the train function I have relized that accoring to [q-learning algorithm](https://en.wikipedia.org/wiki/Q-learning) I need to take the max value of next action. Should my network's last layer must be single nouron or double neoron that one of them is the prob of not pressing space and other one is the prop of pressing the space key. Well according to [this](https://pathmind.com/wiki/deep-reinforcement-learning) page since we have only one action we need one nouron.\n",
    "- However int eh train method I could not figure out where should use the action value in the q-value aupdate formula. thinking onit...\n",
    "- appart from my last project I decided to introduce learning rate hypper parameter here...\n",
    "- since there is only 1 action in action set, the action parameter in train method seems to be irrevalent. so I diceded to remove it. But I am not sure what is going to be consiquences. I think I will see that on [this](https://github.com/hakanonal/geodashml/projects/1#card-37027293) stage\n",
    "- It seems that we have done with the agent class. To test if train method is working correctlly, I need to construct the [environment](https://github.com/hakanonal/geodashml/projects/1#card-37027280)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from os import path\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Conv2DTranspose, BatchNormalization, UpSampling2D, Reshape\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.activations import softmax\n",
    "\n",
    "\n",
    "class agent:\n",
    "    def __init__(self,discount=0.95,exploration_rate=0.9,decay_factor=0.9999, learning_rate=0.1):\n",
    "        self.discount = discount # How much we appreciate future reward over current\n",
    "        self.exploration_rate = exploration_rate # Initial exploration rate\n",
    "        self.decay_factor = decay_factor\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        if(path.exists(self._getModelFilename())):\n",
    "            self.model = load_model(self._getModelFilename())\n",
    "        else:\n",
    "            self.model = Sequential()\n",
    "            self.model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(600,785,4)))\n",
    "            self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "            self.model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "            self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "            self.model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "            self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "            self.model.add(Flatten())\n",
    "            self.model.add(Dense(64,activation=\"relu\"))\n",
    "            self.model.add(Dense(32,activation=\"relu\"))\n",
    "            self.model.add(Dense(1,activation=\"sigmoid\"))\n",
    "            self.model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "    def _getModelFilename(self):\n",
    "        return \"geodash.h5\"\n",
    "\n",
    "    def get_next_action(self, state):\n",
    "        if random.random() < self.exploration_rate: # Explore (gamble) or exploit (greedy)\n",
    "            return self.random_action()\n",
    "        else:\n",
    "            return self.greedy_action(state)\n",
    "\n",
    "    def greedy_action(self, state):\n",
    "        return self.getQ(state) > 0.5\n",
    "    def random_action(self):\n",
    "        return random.random() > 0.5\n",
    "\n",
    "    def getQ(self,state):\n",
    "        state_to_predict = np.expand_dims(state,0)\n",
    "        action_prediction = self.model.predict(state_to_predict)\n",
    "        return action_prediction[0][0]\n",
    "\n",
    "    def train(self, old_state, new_state, reward):\n",
    "        \n",
    "        old_state_prediction = self.getQ(old_state)\n",
    "        new_state_prediction = self.getQ(new_state)\n",
    "\n",
    "        old_state_prediction = ((1-self.learning_rate) * old_state_prediction) + (self.learning_rate * (reward + self.discount * new_state_prediction))\n",
    "\n",
    "        x = np.expand_dims(old_state,0)\n",
    "        y = np.expand_dims(old_state_prediction,0)\n",
    "        self.model.fit(x,y,verbose=0)\n",
    "\n",
    "    def update(self, old_state, new_state, reward):        \n",
    "        self.train(old_state, new_state, reward)\n",
    "        self.exploration_rate *= self.decay_factor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I little bit testing is going on if agent class is ok, how model summary looks like. Make some predictions with checking if class helper methods are working fine..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = agent()\n",
    "\n",
    "for i in range(len(numpt_test_images)):\n",
    "    print(\"%f - %d - %d\"%(\n",
    "        a.getQ(numpt_test_images[i]),\n",
    "        a.greedy_action(numpt_test_images[i]),\n",
    "        a.get_next_action(numpt_test_images[i])\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Build Environment](https://github.com/hakanonal/geodashml/projects/1#card-37027280)\n",
    "\n",
    "29.04.2020\n",
    "- My initial thoght is to make the environment to read the hyper parameters and also using the selenimum we can open game and start the main loop to play constantlly. we may also need to use selenium to read the reward(score) value which is already being shown on the screen. I should also decide the metrics to watch. so let's begin...\n",
    "- currentlly I have incode the hyper parameters to see if the general stuture of the class is working. I will problly convert the configuration to get from wandb on [next](https://github.com/hakanonal/geodashml/projects/1#card-37027287) phase...\n",
    "- I am thinking of how to use the episode parameter. The inner loop that lives as long as one game. So somehow I should detect if our here is going forward, he is crached or he is died. also read the reward.\n",
    "- Ok second thoght, so I will continue the game loop as long as the reward is bigger than the reward that is readed one before. so if the reward is the same with the previus one then we have completed one episode(game). Let's try to code this.\n",
    "- but before that I need to implement the selenium attributes to this class. I get a new error message from the driver. I am not sure whay I am getting that \"HTTPConnectionPool(host='127.0.0.1', port=55773): Max retries exceeded with url: /session/c3a1e26d8e0b10a664932321d5ae2269/element (Caused by NewConnectionError(': Failed to establish a new connection: Errno 61 Connection refused'))\" Before that I was not getting this error. I also need to handle the error stitation, otherwise a lot of chrome aplication will spawn. Now I need to leave, I will continue later on...\n",
    "\n",
    "30.04.2020\n",
    "- Let's see...\n",
    "- Yes! well I did not understand why but the error gone. All I did was to restart the notebook.\n",
    "- Ok where were we?\n",
    "- Alright I have started the start method, start the game by pressing the space on html element. start a loop and succesfully read the score. The score is also being changed during the loop\n",
    "- So the score is being read too fast. see (0,0,1,2,3,4,5,5,6,7,7,8,8,9,9,10,11,11,12,12,12,13,13,14,14,15,15,15,15) same reward may be repeat itself. However when the game is over, I see that the same reward repeats itself much more then others. So let's 5 consecutive repeat of the same reward can be declared as 1 episode. However this may behave differently in different environments. This part is subject for an improvement. \n",
    "- On the other hand, In my experiement I have only printed the reward. I will now try to get the screen shot and maybe sent it to agent prediction so that one loop will be computationally more heavy. I wonder if I see not only repeating reward I may not see some of the rewards. Let's see...\n",
    "- Ok couple of wierd stuff is going on. First, I get the state with readState method and send it to agent's getQ method which should predict though network. I got a shape error. Because I see that when I do not hide the window then the resulation of the screen shot is changing and that's why the resulted numpy array has different shape. I need to transform the screen show every time :/, Other then that if I remove the getq from loop. I read wierd reward values when the window is hidden. I'll give one more try for that\n",
    "- Ok I got it now! To debug I print the element's text one before the reward. I was expecting to see \"Score\" text because the way of the html. After some loop I started to get not score but \"Player FPS\" text. That text was recognized before (check my notes before above). The problem with these htmls they are marked as same class names but they do not have unique id. I need to construct more clever css selectors.\n",
    "- I have read [this](https://stackoverflow.com/questions/2717480/css-selector-for-first-element-with-class) article. I have some idea the different between firts-child and first-type-of. I will try one them by modifying the readScore method. It did not work. FPS value still appear after some time.\n",
    "- I need to see the full html when that FPS value appears. Since that FPS value appears only on hidden mode I need to code a test script. I did that by reading [this](https://www.browserstack.com/guide/get-html-source-of-web-element-in-selenium-webdriver)\n",
    "- yep here is the full html after that. You see that the FPScomes to first position which is the problem.\n",
    "```html\n",
    "<div class=\"react-contextmenu-wrapper\"><div class=\"monitor_monitor-container_2J9gl react-draggable box_box_2jjDp\" style=\"touch-action: none; transform: translate(0px, 0px); top: 6px; left: 170px;\"><div class=\"monitor_default-monitor_2vCcZ\"><div class=\"monitor_row_2y_kM\"><div class=\"monitor_label_ci1ok\">Player: FPS</div><div class=\"monitor_value_3Yexa\" style=\"background: rgb(255, 140, 26);\">21</div></div></div></div></div><div class=\"react-contextmenu-wrapper\"><div class=\"monitor_monitor-container_2J9gl react-draggable box_box_2jjDp\" style=\"touch-action: none; transform: translate(0px, 0px); top: 6px; left: 5px;\"><div class=\"monitor_default-monitor_2vCcZ\"><div class=\"monitor_row_2y_kM\"><div class=\"monitor_label_ci1ok\">Score</div><div class=\"monitor_value_3Yexa\" style=\"background: rgb(255, 140, 26);\">15</div></div></div></div></div><div class=\"react-contextmenu-wrapper\"><div class=\"monitor_monitor-container_2J9gl react-draggable box_box_2jjDp\" style=\"touch-action: none; transform: translate(0px, 0px); top: 4px; left: 355px;\"><div class=\"monitor_default-monitor_2vCcZ\"><div class=\"monitor_row_2y_kM\"><div class=\"monitor_label_ci1ok\">Your Best</div><div class=\"monitor_value_3Yexa\" style=\"background: rgb(255, 140, 26);\">15</div></div></div></div></div>\n",
    "```\n",
    "- I need to distinguish these elements from the labels. [This](https://stackoverflow.com/questions/1520429/is-there-a-css-selector-for-elements-containing-certain-text) article says it is not possible. More research...\n",
    "- The only attribute difference among the elements is the style that is repositionaning them. So maybe I can select via style attribute let give it try...\n",
    "- Yep It worked. However even the only selecting that element itself seems to be heavy computational thing. Even I did not get the q-vlue via network it skips some of the rewards. And adding the q-value makes it even worse. I can get only 2 reward values in each game. :/. I am pretty sure that reading the reward with this much expensive is going to be problem in the future. This will lower the frame rate a lot. We can not expect the agent react fast enough. I need to find a clever way to read or keep track of the reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from PIL import Image\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "class environment:\n",
    "    def __init__(self):\n",
    "        self.config = {\n",
    "            'discount': 0.95,\n",
    "            'exploration_rate': 0.9,\n",
    "            'decay_factor': 0.9999,\n",
    "            'learning_rate': 0.1,\n",
    "            'episode': 100000,\n",
    "            'hide_browser' : 1\n",
    "        }\n",
    "        self.agent = agent(\n",
    "            discount=self.config['discount'],\n",
    "            exploration_rate=self.config['exploration_rate'],\n",
    "            decay_factor=self.config['decay_factor'],\n",
    "            learning_rate=self.config['learning_rate']\n",
    "            )\n",
    "        self.max_reward = 0\n",
    "        self.tot_reward = 0\n",
    "        self.tot_penalty = 0\n",
    "        self.tot_valid = 0\n",
    "        self.episode = self.config['episode']\n",
    "        \n",
    "        self.browser_options = webdriver.ChromeOptions()\n",
    "        if self.config['hide_browser']:\n",
    "            self.browser_options.add_argument('headless')\n",
    "        self.browser_driver = webdriver.Chrome(executable_path=\"/usr/local/bin/chromedriver\", chrome_options=self.browser_options)\n",
    "        self.browser_wait = WebDriverWait(self.browser_driver,timeout=10)\n",
    "\n",
    "        self.browser_driver.get('https://scratch.mit.edu/projects/105500895/fullscreen/')\n",
    "        self.browser_wait.until(lambda d: d.find_element_by_css_selector(\".stage_green-flag-overlay_gNXnv img\"))\n",
    "        flag_element = self.browser_driver.find_element_by_css_selector(\".stage_green-flag-overlay_gNXnv img\")\n",
    "        flag_element.click()\n",
    "        self.browser_wait.until(lambda d: d.find_element_by_css_selector(\".monitor_monitor-container_2J9gl\"))\n",
    "        self.browser_driver.implicitly_wait(20)\n",
    "\n",
    "    def start(self):\n",
    "        html_element = self.browser_driver.find_element_by_tag_name(\"html\")\n",
    "        html_element.send_keys(Keys.SPACE)\n",
    "        self.browser_wait.until(lambda d: d.find_element_by_css_selector(\".monitor_value_3Yexa\"))\n",
    "        while True:\n",
    "            q_value = self.agent.getQ(self.readState())\n",
    "            reward = 1#int(self.readScore())\n",
    "            print('%f - %d'%(q_value,reward))\n",
    "\n",
    "    def readScore(self):\n",
    "        monitor_element = self.browser_driver.find_element_by_css_selector('.monitor-list_monitor-list-scaler_143tA .monitor_monitor-container_2J9gl[style=\"touch-action: none; transform: translate(0px, 0px); top: 6px; left: 5px;\"] .monitor_value_3Yexa')\n",
    "        return monitor_element.text\n",
    "\n",
    "    def readState(self):\n",
    "        image_png = self.browser_driver.find_element_by_css_selector(\"#view\").screenshot_as_png\n",
    "        img = Image.open(io.BytesIO(image_png))\n",
    "        return np.asarray(img)\n",
    "\n",
    "\n",
    "\n",
    "    def end(self):\n",
    "        self.browser_driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "e = environment()\n",
    "try:\n",
    "    e.start()\n",
    "finally:\n",
    "    e.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "02.05.2020 - continue to develop environment\n",
    "\n",
    "- I have decided to ignore the performance problem. Afterall I do notknow the impact of the reward reading process to the overall performance. If I see a performance problem because of low frame rate I will open a card for that. \n",
    "- So I am copy-paste all code both classes in the following section and continue on these versions. \n",
    "- Should I include a random seed hyperparameter for network initilization? Yeah why not? Nevermind it seems to be very complicated. I did not understand. I though it would be easy. However I will add these artices for future refercence. [This](https://machinelearningmastery.com/reproducible-results-neural-networks-keras/), [this](https://stackoverflow.com/questions/50659482/why-cant-i-get-reproducible-results-in-keras-even-though-i-set-the-random-seeds) and [this](https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development)\n",
    "- I will continue my first strategy: check the reward change to see if game is continuing...\n",
    "- Ok I have completed the full environment. Theoritaclly of courese. However the single step in one episode is slow that I even can not detect if the episode has finisshed or not. I defintelly need new strategy or keeping a clever way to keep track of reward.\n",
    "- The game flowwing though underneeth my program. My program's sight praticclly useless. I am kind of stuck here... Just looking into code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from os import path\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Conv2DTranspose, BatchNormalization, UpSampling2D, Reshape\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.activations import softmax\n",
    "\n",
    "\n",
    "class agent:\n",
    "    def __init__(self,discount=0.95,exploration_rate=0.9,decay_factor=0.9999, learning_rate=0.1):\n",
    "        self.discount = discount # How much we appreciate future reward over current\n",
    "        self.exploration_rate = exploration_rate # Initial exploration rate\n",
    "        self.decay_factor = decay_factor\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        if(path.exists(self._getModelFilename())):\n",
    "            self.model = load_model(self._getModelFilename())\n",
    "        else:\n",
    "            self.model = Sequential()\n",
    "            self.model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(600,785,4)))\n",
    "            self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "            self.model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "            self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "            self.model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "            self.model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "            self.model.add(Flatten())\n",
    "            self.model.add(Dense(64,activation=\"relu\"))\n",
    "            self.model.add(Dense(32,activation=\"relu\"))\n",
    "            self.model.add(Dense(1,activation=\"sigmoid\"))\n",
    "            self.model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "    def _getModelFilename(self):\n",
    "        return \"geodash.h5\"\n",
    "\n",
    "    def get_next_action(self, state):\n",
    "        if random.random() < self.exploration_rate: # Explore (gamble) or exploit (greedy)\n",
    "            return self.random_action()\n",
    "        else:\n",
    "            return self.greedy_action(state)\n",
    "\n",
    "    def greedy_action(self, state):\n",
    "        return self.getQ(state) > 0.5\n",
    "    def random_action(self):\n",
    "        return random.random() > 0.5\n",
    "\n",
    "    def getQ(self,state):\n",
    "        state_to_predict = np.expand_dims(state,0)\n",
    "        action_prediction = self.model.predict(state_to_predict)\n",
    "        return action_prediction[0][0]\n",
    "\n",
    "    def train(self, old_state, new_state, reward):\n",
    "        \n",
    "        old_state_prediction = self.getQ(old_state)\n",
    "        new_state_prediction = self.getQ(new_state)\n",
    "\n",
    "        old_state_prediction = ((1-self.learning_rate) * old_state_prediction) + (self.learning_rate * (reward + self.discount * new_state_prediction))\n",
    "\n",
    "        x = np.expand_dims(old_state,0)\n",
    "        y = np.expand_dims(old_state_prediction,0)\n",
    "        self.model.fit(x,y,verbose=0)\n",
    "\n",
    "    def update(self, old_state, new_state, reward):        \n",
    "        self.train(old_state, new_state, reward)\n",
    "        self.exploration_rate *= self.decay_factor\n",
    "\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from PIL import Image\n",
    "import io\n",
    "import numpy as np\n",
    "import yaml\n",
    "\n",
    "class environment:\n",
    "    def __init__(self):\n",
    "        self.config = {\n",
    "            'discount': 0.95,\n",
    "            'exploration_rate': 0.9,\n",
    "            'decay_factor': 0.9999,\n",
    "            'learning_rate': 0.1,\n",
    "            'episode': 100000,\n",
    "            'hide_browser' : 1\n",
    "        }\n",
    "        self.agent = agent(\n",
    "            discount=self.config['discount'],\n",
    "            exploration_rate=self.config['exploration_rate'],\n",
    "            decay_factor=self.config['decay_factor'],\n",
    "            learning_rate=self.config['learning_rate']\n",
    "            )\n",
    "        self.reward = 0\n",
    "        self.max_reward = 0\n",
    "        self.tot_reward = 0\n",
    "        self.tot_penalty = 0\n",
    "        self.tot_valid = 0\n",
    "        self.episode = self.config['episode']\n",
    "        \n",
    "        self.browser_options = webdriver.ChromeOptions()\n",
    "        if self.config['hide_browser']:\n",
    "            self.browser_options.add_argument('headless')\n",
    "        self.browser_driver = webdriver.Chrome(executable_path=\"/usr/local/bin/chromedriver\", chrome_options=self.browser_options)\n",
    "        self.browser_wait = WebDriverWait(self.browser_driver,timeout=10)\n",
    "\n",
    "        self.browser_driver.get('https://scratch.mit.edu/projects/105500895/fullscreen/')\n",
    "        self.browser_wait.until(lambda d: d.find_element_by_css_selector(\".stage_green-flag-overlay_gNXnv img\"))\n",
    "        flag_element = self.browser_driver.find_element_by_css_selector(\".stage_green-flag-overlay_gNXnv img\")\n",
    "        flag_element.click()\n",
    "        self.browser_wait.until(lambda d: d.find_element_by_css_selector(\".monitor_monitor-container_2J9gl\"))\n",
    "        self.browser_driver.implicitly_wait(20)\n",
    "        self.browser_html_element = self.browser_driver.find_element_by_tag_name(\"html\")\n",
    "\n",
    "    def start(self):\n",
    "        self.playAction(1) #to start the game\n",
    "        self.browser_wait.until(lambda d: d.find_element_by_css_selector(\".monitor_value_3Yexa\"))        \n",
    "        \n",
    "        for episode in range(1,self.config['episode']+1):\n",
    "            self.reward = 0\n",
    "            past_reward = 0\n",
    "            while True:\n",
    "                old_state = self.readState()\n",
    "                action_to_play = self.agent.get_next_action(old_state)\n",
    "                self.playAction(action_to_play)\n",
    "                new_state = self.readState()\n",
    "                self.reward = int(self.readScore())\n",
    "                self.agent.update(old_state,new_state,self.reward)\n",
    "                self.tot_reward += self.reward\n",
    "                self.max_reward = max(self.max_reward,self.reward)\n",
    "                if(self.reward == past_reward):\n",
    "                    self.tot_penalty += 1\n",
    "                    break\n",
    "                else:\n",
    "                    self.tot_valid += 1\n",
    "                past_reward = self.reward\n",
    "            \n",
    "                metrics = {\n",
    "                    'valid-total' : self.tot_valid,\n",
    "                    'valid-avarage' : self.tot_valid/episode,\n",
    "                    'penalty-total' : self.tot_penalty,\n",
    "                    'penalty-avarage' : self.tot_penalty/episode,\n",
    "                    'exploration-rate' : self.agent.exploration_rate,\n",
    "                    'max-reward' : self.max_reward,\n",
    "                    'tot-reward' : self.tot_reward,\n",
    "                    'avg-reward' : self.tot_reward/episode,\n",
    "                    'reward':self.reward,\n",
    "                }\n",
    "                print(\"%d\"%metrics['reward'])\n",
    "\n",
    "    def readScore(self):\n",
    "        monitor_element = self.browser_driver.find_element_by_css_selector('.monitor-list_monitor-list-scaler_143tA .monitor_monitor-container_2J9gl[style=\"touch-action: none; transform: translate(0px, 0px); top: 6px; left: 5px;\"] .monitor_value_3Yexa')\n",
    "        return monitor_element.text\n",
    "\n",
    "    def readState(self):\n",
    "        image_png = self.browser_driver.find_element_by_css_selector(\"#view\").screenshot_as_png\n",
    "        img = Image.open(io.BytesIO(image_png))\n",
    "        return np.asarray(img)\n",
    "\n",
    "    def playAction(self,action):\n",
    "        if action:\n",
    "            self.browser_html_element.send_keys(Keys.SPACE)\n",
    "\n",
    "\n",
    "    def end(self):\n",
    "        self.browser_driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "11\n15\n3\n1\n4\n7\n8\n9\n11\n12\n13\n15\n15\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-7fd2539b71cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-36b70ce62072>\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadScore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtot_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_reward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-36b70ce62072>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, old_state, new_state, reward)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexploration_rate\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecay_factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-36b70ce62072>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, old_state, new_state, reward)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_state_prediction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3727\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3729\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \"\"\"\n\u001b[0;32m-> 1551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "e = environment()\n",
    "try:\n",
    "    e.start()\n",
    "finally:\n",
    "    e.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37264bitb4053a4b20a84f3abb1bff9c403e377a",
   "display_name": "Python 3.7.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}